\documentclass[oneside]{projectpaper} %%Change `twoside' to `oneside' if you are printing only on the one side of each sheet.
\usepackage{amsmath}
\usepackage{amssymb,mathtools}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}
\graphicspath{}

\studname{Alex Wong}
\studmail{asw2181@columbia.edu}
\coursename{COMS E6232}
\uni{asw2181}

\begin{document}
\maketitle
\begin{abstract}
We look at the state of the art algorithm for the general case of the Asymmetric Traveling Salesman problem and its current known inapproximability bounds. The currently best algorithm, by Asadpour et al.\cite{AGM10}, details an $O$(log $n$ / log log $n$)-approximation algorithm for the general case of ATSP for costs satisfying the triangle inequality. The best known inapproximability bounds is a proof by Karpinski et al.\cite{KLS15} that gives us a hardness of approximation bounds of 75 / 74 for ATSP.
\end{abstract}
%%%%%%%%%%%%%%%%%%%%Section 1 Introduction%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
The traveling salesman problem is one of the most well-known and studied problems in theoretical computer science and operations research. Many real world problems, from delivery routing to latency networks to DNA sequencing, can be framed as some variation of the traveling salesman problem. In this paper, we look at the current state of the art algorithms for the asymmetric traveling salesman problem (ATSP) where the costs satisfy the triangle inequality (costs from some point $u$ to point $w$ is at most the cost from point $u$ to point $v$ + cost from point $v$ to point $w$) but the cost from some point $u$ to point $v$ does not necessarily equal the cost from point $v$ to point $u$. From this description, we can frame the symmetric traveling salesman problem (the cost from some point $u$ to point $v$ does equal the cost from point $v$ to point $u$) as a special case of ATSP. \newline
\indent In Section 2, we provide an overview of an $O$(log $n$ / log log $n$)-approximation algorithm for the general case of ATSP for costs satisfying the triangle inequality which comes from the work of Asadpour et al. \cite{AGM10}. The $O$(log $n$ / log log $n$)-approximation algorithm introduces the concept of the "thinness" of a spanning tree where the thinness correlates with the approximation bound of ATSP. This is an improvement over the decades-long standing $\Theta$(log$n$)-approximation bound given by the work of Frieze et al. \cite{FGM83}. \newline
\indent In Section 3, we give an overview of the tools and methods that Karpinski et al. \cite{KLS15} used to derive the current best 75/74 inapproximability bounds for ATSP. They introduce a tool called the ``bi-wheel amplifier'' that improves the number of satisfied constraints for a certain type of CSP. This improvement triumphs the 117/116 inapproximability bound given by Papadimitriou and Vempala \cite{PV06}.  \newline
\indent Lastly, in Section 4, we will wrap up the paper with opinions on the next steps towards a constant factor approximation algorithm for the general case of ATSP and briefly mention some special cases of ATSP that have achieved better approximation factors than the current best for the general case.

%%%%%%%%%%%%%%%%%%%%Section 2 An O logn / loglogn approx algo%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{An $O$(log $n$ / log log $n$)-approximation algorithm for ATSP}
In this section, we dive deeply into the works of Asadpour et al. \cite{AGM10} and look through all the methods and theorems to see how the $O$(log $n$ / log log $n$)-approximation algorithm has resulted. Algorithm 1 details the $O$(log $n$ / log log $n$)-approximation algorithm for ATSP (from \cite{AGM10} section 1). 
\begin{algorithm}
\caption{An $O$(log $n$ / log log $n$)-approximation algorithm for ATSP \cite{AGM10}}
\begin{algorithmic}
\STATE{\textbf{Input:} A set $V$ consisting of $n$ points and a cost function $c$ : $V \times V \rightarrow \mathbb{R}^+$ where the costs satisfy the triangle inequality.}
\STATE{\textbf{Output:} An $O$($\frac{logn}{log log n}$)-approximation to the ATSP instance described by the inputs $V$ and $c$.}
\STATE{}
\STATE{1. Solve the Held-Karp LP relaxation of the ATSP instance to get an optimum extreme point solution $\textbf{x}^*$. Create a symmetrized and scaled down version of $\textbf{x}^*$ from (5) and define it as $\textbf{z}^*$. $\textbf{z}^*$ is a vector which can be interpretted as a point in the relative interior (not touching any boundaries) of the spanning tree polytope $P$ created from an undirected graph supported by $\textbf{x}^*$ where we disregard the directions of arcs. Another way to think of $\textbf{z}^*$ is the marginal probabilities on the edges $z_{e}^{*}$ of an exponential  distribution on spanning trees $\tilde{p}(.)$.}
\STATE{}
\STATE{2. Let $E$ be the support graph of $\textbf{z}^*$ where we disregard the directions of arcs. We then find weights \{$\tilde{\gamma}\}_{e \in E}$ such that $\tilde{p}(T)$ is approximately proportional to $\text{exp}(\sum\limits_{e \in T}\tilde{\gamma}_e)$ where, for any edge $e \in E$, $\sum\limits_{T \in \mathcal{T} : T \ni e}\tilde{p}(T) \leq (1 + \epsilon)z_{e}^{*}$, for a small value $\epsilon$.}
\STATE{}
\STATE{3. Sample $2 \lceil logn \rceil$ spanning trees $T_i$ from $\tilde{p}(.)$. Orient all the edges of the sampled spanning trees to minimize its cost since the spanning tree only has to be weakly connected. Let $\overrightarrow{T}$ represent an oriented tree. Let $T^{*}$ be the spanning tree that has minimum total cost amongst all the sample spanning trees.}
\STATE{}
\STATE{4. Find a minimum cost integral circulation that contains the oriented tree $\overrightarrow{T}^*$. This will give us a multigraph. Shortcut the multigraph to get a Eulerian tour and output the result.}
\end{algorithmic}
\end{algorithm}
\newline
Throughout the rest of this section, we will divide each step of the algorithm into subsection(s) to describe it in more detail and/or prove its correctness.

%Section 2.1 Preliminaries and Notation%
\subsection{Preliminaries and Notation}
In this subsection, we describe much of the notation that will be used throughout the rest of this section. We will also define the notion of "thinness" of a spanning tree. \newline
\indent Let $a = (u, v)$ be the arc (directed edge) from $u$ to $v$ and let $e = (u, v)$ be an undirected edge. Then, A (resp. E) is the set of arcs (resp. edges) in a directed (resp. undirected) graph G. \newline
\indent For a given function $f : A \rightarrow \mathbb{R}$, the cost of $f$ is defined as $c(f) := \sum\limits_{a \in A}c(a)f(a)$. For some set of arcs $S \subseteq A$, we define $f(S) := \sum\limits_{a \in S}f(a)$. The same notation is used for the edge set $E$ of an undirected graph. \newline
\indent For a subset of vertices $U \subseteq V$, we define
\begin{equation*}
  \delta^{+}(U) := \{a = (u, v) \in A : u \in U, v \notin U\},
\end{equation*}
\begin{equation*}
  \delta^{-}(U) := \{a = (u, v) \in A : u \notin U, v \in U\},
\end{equation*}
\begin{equation*}
  A(U) := \{a = (u, v) \in A : u \in U, v \in U\}
\end{equation*}
\begin{equation*}
  \delta(U) := \delta^{+}(U) \cup \delta^{-}(U),
\end{equation*}
to be the set of arcs that are leaving, entering, contained in $U$, and total of incoming and outgoing arcs, respectively. We also define $\delta^{+}(v) :=\delta^{+}(\{v\})$ and $\delta^{-}(v) :=\delta^{-}(\{v\})$ for each single vertex $v$. For an undirected graph, $\delta(U)$ represents the set of edges with just one endpoint in $U$, and $E(U)$ represents the edges that are contained within $U$. Lastly, all $log$ in equations represents the natural logarithm. \newline
\indent We say that a spanning tree $T$ is $\alpha$-thin with respect to $\textbf{z}$  if and only if: 
\begin{equation*}
|T \cap \delta(U)| \leq \alpha \cdot z(\delta(U)) \indent \forall U \subset V
\end{equation*}
which essentially states that for all the subsets of vertices $U$ of the spanning tree T, the number of edges in the cut between $U$ and $V \backslash U$ must be $\leq$ the thinness multiplied by the cost of the edges in the cut. \newline
\indent We also say that T is $(\alpha, s)$-thin with respect to $\textbf{z}$ if and only if it is $\alpha$-thin and $c(T) \leq s \cdot OPT_{HK}$, which essentially states that the cost of $T$ is at most $s$ times the cost of the optimal Held-Karp solution.
\indent By showing that a spanning tree is "thin", we would be able to get a Eulerian augmentation of the spanning tree where the total cost is within a factor of $\alpha$ of the cost $OPT_{HK}$, which implies within the same factor of the optimum solution.

%Section 2.2 The Held-Karp Relaxation%
\subsection{The Held-Karp Relaxation}
We detail and prove step 1 of the algorithm. Given an ATSP instance with a cost function $c : V \times V \rightarrow \mathbb{R}^+$, we can get a lower bound on the optimal value of ATSP by the following linear programming (LP) relaxation defined on the complete bidirected graph over the vertex set $V$:
\begin{equation}
  min \sum\limits_{a}c(a)x_a
\end{equation}
\begin{equation}
  s.t. \ \ \textbf{x}(\delta^{+}(U)) \geq 1 \indent \forall U \subset V,
\end{equation}
\begin{equation}
  \textbf{x}(\delta^{+}(v)) = \textbf{x}(\delta^{-}(v)) = 1 \indent \forall v \in V,
\end{equation}
\begin{equation*}
  x_a \geq 0 \indent \forall a.
\end{equation*}
This relaxation is accurate since (2) makes sure that all the vertices are strongly connected and (3) makes sure that the indegree = outdegree for all vertices of $V$ which makes this a Eulerian graph. This relaxation is known as the Held-Karp relaxation \cite{HK70} and it's well-known that an optimal solution $\textbf{x}^*$ to the relaxation can be computed in polynomial time (using the ellipsoid method). Thus, we can say that $c(\textbf{x}^*) = OPT_{HK}$. Also, notice that (3) implies that any feasible solution \textbf{x} satisfies
\begin{equation}
  \textbf{x}(\delta^{+}(U)) =   \textbf{x}(\delta^{-}(U)) \indent \forall U \subset V.
\end{equation}

%Section 2.3 Proving z* \in Relative Interior of P%
\subsection{Proving z* $\in$ Relative Interior of P}
\indent As mentioned in the algorithm, $\textbf{z}^*$ is the symmetreized and scaled down version of $\textbf{x}^*$. So, we formally define
\begin{equation}
 \textbf{z}_{\{u, v\}}^{*} := \frac{n-1}{n}(\textbf{x}_{uv}^* + \textbf{x}_{vu}^*).
\end{equation}
\indent As mentioned in step 2 of the algorithm , we let $E$ be the support of $\textbf{z}^*$. We also let $A$ be the support of $\textbf{x}^*$ where $A = \{(u, v) : x_{uv}^{*} > 0 \}$. We also define the costs of all edges $e \in E$ where $c(e) = min \{c(a) : a \in \{(u, v), (v, u)\} \cap A \}, \ \forall e \in E$. This shows that $c(\textbf{z}*) < c(\textbf{x}*)$. Now we can prove the rest of step 1 of the algorithm and define a lemma:
\newline
\newline
\textbf{Lemma 2.1 (\cite{AGM10}, section 3):} The vector $\textbf{z}^*$ belongs to the relative interior of the spanning tree polytope P.
\newline
\newline
\textbf{Proof:} From the characterization of the base polytope of a matroid given by Edmonds\cite{Edm71}, it follows that the spanning tree polytope P is defined by the following inequalities (see Corollary 50.7c of \cite{Sch03}):
\begin{equation}
P = \{z \in \mathbb{R}^E : z(E) = |V| - 1,
\end{equation}
\begin{equation}
z(E(U)) \leq |U| - 1 \indent \forall U \subset V \ where \ U \neq \emptyset,
\end{equation}
\begin{equation}
z_e \geq 0 \indent \forall e \in E.\}
\end{equation}
Thus, for some vector z to belong in the relative interior of P, it must satisfy both inequalities (7) and (8) strictly ($z(E(U)) < |U| - 1$ and $z_e > 0$). \newline
From the above equations, we see that $\textbf{z}^*$ satisfies constraint (6) because:
\begin{equation*}
\forall v \in V, \textbf{x}^*(\delta^+(v)) = 1 \ \ \Rightarrow \ \ \textbf{x}^*(A) = n = |V| \ \ \Rightarrow \ \ \textbf{z}^*(E) = n - 1 = |V| - 1
\end{equation*}
Now, consider any set $U \subset V$ where $U \neq \emptyset$. We get that
\begin{equation*}
\sum\limits_{v \in U}\textbf{x}^*(\delta^+(v)) \ = \  |U| \ = \ \textbf{x}^*(A(U)) + \textbf{x}^*(\delta^+(U)) \ \geq \ \textbf{x}^*(A(U)) + 1
\end{equation*}
\begin{equation*}
\Rightarrow \ \textbf{x}^*(A(U)) \ \leq \ |U| - 1
\end{equation*}
We see that $\textbf{x}^*$ satisfies constraints (2) and (3), so we can use equation (5) to show
\begin{equation*}
\textbf{z}^*(E(U)) = \frac{n - 1}{n}\textbf{x}^*(A(U)) < \textbf{x}^*(A(U)) \leq |U| - 1
\end{equation*}
\begin{equation*}
\Rightarrow \ \textbf{z}^*(E(U)) < |U| - 1
\end{equation*}
where we see that $\textbf{z}^*$ satisfies inequality (7) strictly. Also, since E is the support of $\textbf{z}^*$, that means $\textbf{z}^*$ also satisfies inequality (8) strictly. Thus, we can conclude that $\textbf{z}^*$ is in the relative interior of P. \hfill\qed
\newline
\newline
We also make a note that since $\textbf{x}^*$ is an optimum extreme point solution, it's known that its support $A$ has at most $3n - 4$ arcs (see \cite{Goe06}, Theorem 15). Also, $\textbf{x}^*$ can be expressed as the unique solution (due to global minima of a convex program) of an invertible system with only 0 - 1 coefficients, then every entry $\textbf{x}_{a}^{*}$ is rational with an integral numerator and denominator bounded by $2^{O(nlogn)}$, thus we can say that $\textbf{z}_{min}^{*} > 2^{-O(nlogn)}$ (see \cite{AGM10} section 3).

%Section 2.4 Thin Tree Properties%
\subsection{Thin Trees, Flow Circulation, and Their Relation to ATSP}
A keyf element to the algorithm of approximating ATSP is the notion of thin trees. In section 2.1, we briefly described what "thinness" means and what properties a thin tree has. We will now detail it further to showcase why thin trees are crucial to this approximation algorithm and prove its correctness. \newline
\indent We jump to step 4 of the algorithm to prove it. First, though, a theorem that is required for the proof of correctness of thin trees is Hoffman's circulation theorem \cite{Sch03}. A circulation is essentially any function $f : A \rightarrow \mathbb{R}$ such that $f(\delta^+(v)) = f(\delta^-(v))$ for each vertex $v \in V$. Hoffman's circulation theorem gives a necessary and sufficient condition for the existence of circulation on the arcs subject to the arcs' lower and upper capacities. The following is Hoffman's circulation theorem (we won't prove Hoffman's circulation theorem; see \cite{Sch03}, Theorem 11.2 for the proof):
\newline
\newline
\textbf{Theorem 2.2 (Hoffman's Circulation Theorem \cite{Sch03}):} Given lower and upper capacities $l, u : A \rightarrow \mathbb{R}$, there exists a circulation $f$ satisfying $l(a) \leq f(a) \leq u(a)$ for all $a \in A$ if and only if:
\newline \indent 1. $l(a) \leq u(a)$ for all $a \in A$ and
\newline \indent 2. for all subsets $U \subseteq V$, we have $l(\delta^-(U)) \leq u(\delta^+(U))$.
\newline Furthermore, if $l$ and $u$ are integer-valued, $f$ can be chosen to be integer-valued too.
\newline
\newline
Given this theorem on circulation, we now provide a theorem that proves that the ability to find an $(\alpha, s)$-thin spanning tree with respect to $\textbf{z}^*$, for some $\alpha$ and $s$, translates directly into an ability to obtain an $(2\alpha + s)$-approximation to ATSP:
\newline
\newline
\textbf{Theorem 2.3 (\cite{AGM10}, section 4):} Let $\textbf{x}^*$ be an optimal solution to the Held-Karp relaxation and $\textbf{z}^*$ to be the symmetrized and scaled down version of $\textbf{x}^*$ defined in (5). if $T^*$ is an $(\alpha, s)$-thin spanning tree with respect to $\textbf{z}^*$ for some $\alpha$ and $s$, then we can find, in polynomial-time, a Hamiltonian cycle whose cost is at most $(2\alpha + s)c(\textbf{x}^*) = (2\alpha + s)OPT_{HK} \leq (2\alpha + s)OPT$.
\newline
\newline
\textbf{Proof:} We first orient the edges $(u, v)$ of $T^*$ to minimize the cost of $T^*$ where each arc $a \in T^* = min\{c(a) : a \in \{(u, v), (v, u)\} \cap A\}$, and we denote the oriented tree as $\overrightarrow{T}^*$. Notice that by the definition of the undirected cost function that we defined back in section 2.3, we have $c(\overrightarrow{T}^*) = c(T^*)$. \newline
\indent Lets now consider an augmentation of minimum cost to make $\overrightarrow{T}^*$ an Eulerian directed graph. To get the augmentation, we can translate it as a minimum cost circulation problem with integral lower capacities and a bound on the upper capacities bsed on what we want to prove here in Theorem 2.3. So, we first set the lower capacities of the arcs by 
\begin{equation*}
l(a) =
  \begin{cases}
    1 \indent a \in \overrightarrow{T}^* \\
    0 \indent a \notin \overrightarrow{T}^* \\
  \end{cases}
\end{equation*}
and the minimum circulation problem is $min\{c(f) : f$ is a circulation and $f(a) \geq l(a), \ \forall a \in A\}$. This minimum circulation problem is well known so we do not prove it here (see \cite{Sch03}, Corollary 12.2a for the proof). The minimum cost circulation problem's proof will give us an optimal integral circulation solution $f^*$ in polynomial time. When we augment this solution with $\overrightarrow{T}^*$, we end up with a directed multigraph that is Eulerian (indegree = outdegree $\forall v \in V$), which we will define as $H$. Thus, $H$ is strongly connected so we can take a Eulerian tour and shortcut it to get a Hamiltonian cycle of cost $\leq c(f^*)$. \newline
\indent The last part of this proof is showing an upper bound on $c(f^*)$. As we stated in our theorem, we are looking for cost $\leq (2\alpha + s)c(\textbf{x}^*)$, meaning that $c(f^*) \leq (2\alpha + s)c(\textbf{x}^*)$. So, we can define the upper capacities of the arc by
\begin{equation*}
u(a) =
  \begin{cases}
    1 + 2\alpha x_{a}^{*} \indent a \in \overrightarrow{T}^* \\
    2\alpha x_{a}^{*} \indent \indent \  a \notin \overrightarrow{T}^*. \\
  \end{cases}
\end{equation*}
Now, we claim that there $\exists$ a circulation g where $l(a) \leq g(a) \leq u(a) \ \forall a \in A$. This implies that
\begin{equation*}
c(f^*) \leq c(g) \leq c(u) = c(\overrightarrow{T}^*) + 2\alpha c(\textbf{x}^*) \leq (2\alpha + s)c(\textbf{x}^*),
\end{equation*}
which gives us the bound we want for $c(f^*)$. To prove this claim, we see that the $\alpha$-thinness of $T^*$ implies that for any $U \subseteq V$, the number of arcs of $\overrightarrow{T}^*  \in \delta^-(U) \leq \alpha \textbf{z}^*(\delta(U))$, regardless of the orientation where $T^*$ oriented to $\overrightarrow{T}^* $. Thus, we can say that
\begin{equation*}
  l(\delta^-(U)) \leq \alpha \textbf{z}^*(\delta(U))
\end{equation*}
and by using a combination of (4) and (5), we can say that
\begin{equation*}
  l(\delta^-(U)) \leq \alpha \textbf{z}^*(\delta(U)) < 2\alpha \textbf{x}^*(\delta^-(U)).
\end{equation*}
We also know that 
\begin{equation*}
 2\alpha \textbf{x}^*(\delta^+(U)) \leq u(\delta^+(U)). 
\end{equation*}
By equation (4), we can say that 
\begin{equation*}
2\alpha \textbf{x}^*(\delta^+(U)) = 2\alpha \textbf{x}^*(\delta^-(U)).
\end{equation*}
So, we can finally conclude 
\begin{equation*}
  l(\delta^-(U)) \leq 2\alpha \textbf{x}^*(\delta^-(U)) = 2\alpha \textbf{x}^*(\delta^+(U)) \leq u(\delta^+(U))
\end{equation*}
\begin{equation*}
\Rightarrow l(\delta^-(U)) \leq u(\delta^+(U))
\end{equation*}
for any $U \subseteq V$, thus showing that the circulation g does exist, which concludes the proof of this theorem. \hfill\qed

%Section 2.5 Maximum Entropy Distribution%
\subsection{Maximum Entropy Distribution}
In this subsection, we detail the approach on how to find an entropy distribution $p(.)$. We let $\mathcal{T}$ represent the collection of all spanning trees of $G = (V, E)$ and let \textbf{z} represent some arbitrary point in the relative interior of the spanning tree polytope $P$ of $G$. We now let $p^*(.)$ be the maximum entropy distribution with respect to the marginal probabilites imposed by \textbf{z}. This is considered the optimum solution of the following convex program:
\begin{equation}
inf \indent \sum\limits_{T \in \mathcal{T}}p(T)logp(T)
\end{equation}
\begin{equation*}
s.t. \indent \sum\limits_{T \ni e}p(T) = z_e \indent \forall e \in E,
\end{equation*}
\begin{equation*}
\indent \ \ p(T) \geq 0 \indent \forall T \in \mathcal{T}.
\end{equation*}
This convex program is feasible since \textbf{z} is in the relative interior of the spanning tree polytope P and as the objective function is bounded and the feasible region is compact, the infinum (greatest lower bound) is found, which shows the existence of an optimal solution $p^*(.)$. Also, since the objective function is stricly convex (any local minima is essentially a global minima), this implies that the maximum entropy distribution $p^*(.)$ is unique. $OPT_{CP}$ will represent the optimum value of the convex program (9). \newline
\indent We define $p(T)$ as the probability of sampling a spanning tree $T$ in $p(.)$. Also, for any feasible solution $p(.)$, we have $\sum\limits_{T}p(T) = 1$ because
\begin{equation*}
n - 1 = \sum\limits_{e \in E}z_e = \sum\limits_{e \in E}\sum\limits_{T \ni e}p(T) = (n - 1)\sum\limits_{T}p(T)
\end{equation*}
based on the constraints given by convex program (9). We now define a theorem: 
\newline
\newline
\textbf{Theorem 2.4 (see \cite{AGM10}, section 5):} Given a vector \textbf{z} in the relative interior of the spanning tree polytope $P$ of $G = (V, E)$, $\exists \ \gamma_{e}^{*} \ \forall e\in E$, such that if we sample a spanning tree $T$ of $G$ according to $p^*(T) := e^{\gamma^*(T)}$, then Pr[$e \in T$] $= z_e \ \ \forall e \in E$.
\newline
\newline
\textbf{Proof:} Now, assuming that \textbf{z} is in the relative interior of the spanning tree polytope P, we want to now show that show that $p^*(T) > 0 \ \ \forall T \in \mathcal{T}$ and that $p^*(T)$ gives a simple exponential formula. To show this, we find the Lagrange to the convex program (9). Thus, $\forall e \in E$, we have a Lagrange multiplier $\delta_e$ (not to be confused with $\delta^-$ and $\delta^+$ which represents incoming and outgoing edges, respectively) to the constraint corresponding to the marginal probability $z_e$, and we define the Lagrange function as
\begin{equation*}
L(p,\delta) = \sum\limits_{T \in \mathcal{T}}p(T)logp(T) - \sum\limits_{e \in E}\delta_e\bigg(\sum\limits_{T \ni e}p(T) - z_e\bigg) 
\end{equation*}
which can be rewritten as
\begin{equation*}
L(p,\delta) = \sum\limits_{e \in E}\delta_ez_e + \sum\limits_{T \in \mathcal{T}}\bigg(p(T)logp(T)  - p(T) \sum\limits_{e \in T}\delta_e\bigg)
\end{equation*}
thus making the Lagrange dual to the convex program (9)
\begin{equation}
\sup_{\delta}\inf_{p \geq 0}L(p,\delta)
\end{equation}
We solve the inner infimum. Since each of the contributions of $p(T)$ is seperable, then $\forall T \in \mathcal{T}$, $p(T)$ must minimize the convex function $p(T)logp(T) - p(T)\delta(T)$ where $\delta(T) = \sum\limits_{e \in T}\delta_e$. Now, taking partial derivatives of the convex function with respect to $p(T)$, we get
\begin{equation*}
1 + logp(T) - \delta(T) = 0
\end{equation*}
\begin{equation*}
logp(T) = \delta(T) - 1
\end{equation*}
\begin{equation}
p(T) = e^{\delta(T) - 1}
\end{equation}
So, the inner infimum
\begin{equation*}
\inf_{p \geq 0}L(p,\delta) = \sum\limits_{e \in E}\delta_{e}z_e - \sum\limits_{T \in \mathcal{T}}e^{\delta(T) - 1}
\end{equation*}
and we use a change of variables to define $\gamma_e= \delta_e - \frac{1}{n-1}$ for $e \in E$, the dual (10) can be rewritten as
\begin{equation}
\sup_{\gamma}\bigg(1 + \sum\limits_{e \in E}z_e\gamma_e - \sum\limits_{T \in \mathcal{T}}e^{\gamma(T)}\bigg).
\end{equation}
This tells us that vector \textbf{z} being in the relative interior of P satisfies Slater's condition and being convex implies that the supremum of (12) is given by $\gamma^*$, and thus the Lagrange dual value = the optimum value $OPT_{CP}$. Also, given that we have the primal optimum solution $p^*$, any dual optimum solution $\gamma^*$ must satisfy
\begin{equation}
L(p,\gamma^*) \geq L(p^*,\gamma^*) \geq L(p^*,\gamma)
\end{equation}
for any $p \geq 0$ and any $\gamma$. Thus, $p^*$ is the unique minimizer of $L(p,\gamma^*)$ and from equation (11),
\begin{equation}
p^*(T) = e^{\gamma^*(T)}
\end{equation}
thus proving the theorem. \hfill \qed
\newline
\newline
\indent To find the weights $\tilde{\gamma}_{e}, \forall e \in E$, requires lengthy non-trivial calculations which will not be detailed within this paper. There are two methods to find $\tilde{\gamma}_{e}$'s: either the combinatorial approach (see \cite{AGM10} section 7) where the basic idea is to iteratively tweak the weights of $\tilde{\gamma}_{e}$ until their marginals $= (1 + \epsilon/2)z_e$ or the ellipsoid method (see \cite{AGM10} section 8) where the basic idea is to find a near optimal solution of the dual of the convex program. We will define a theorem that utilizes those weights $\tilde{\gamma}_{e}$ since it is crucial for subsequent theorems and proofs:
\newline
\newline
\textbf{Theorem 2.5 (see \cite{AGM10}, section 5):} Given \textbf{z} in the spanning tree polytope of $G = (V, E)$ and some $\epsilon > 0$, values $\tilde{\gamma}_e \ \ \forall e \in E$ can be found, so that if we define the exponential family distribution
\begin{equation*}
\tilde{p}(T) := \frac{1}{P}exp(\sum\limits_{e \in T}\tilde{\gamma}_e)
\end{equation*}
for all $T \in \mathcal{T}$ where 
\begin{equation*}
P := \sum\limits_{T \in \mathcal{T}}exp(\sum\limits_{e \in T}\tilde{\gamma}_e
\end{equation*}
then, for every edge $e \in E$,
\begin{equation*}
\tilde{z}_e := \sum\limits_{T \in \mathcal{T} : T \ni e}\tilde{p}(T) \leq (1 + \epsilon)z_e.
\end{equation*}
This theorem essentially states that the marginal probabilities of \textbf{z} are approximately preserved. Also, we set $\epsilon = 0.2$ and $z_{min} = 2^{-O(nlogn)}$ to be used for calculations in subsequent subsections. \newline
\indent We now define the notion of $\lambda$-random trees, which essentially correlate with spanning trees. We say that a $\lambda$-random tree $T$ of $G$ is a spanning tree T chosen from the set of all spanning trees of $G$ with probability proportional to $\prod\limits_{e \in T}\lambda_e$ where $\lambda_e \geq 0$ for $e \in E$. For the case where all of the $\lambda_e$'s are equal, the result is a uniform spanning tree of G. For rational $\lambda_e$'s, a $\lambda$-random spanning tree in G correlates to a uniform spanning tree in a multigraph obtained from G by allowing the multiplicity of edge $e$ be proportional to $\lambda_e$. From Theorem 2.5, we see that if we sample a tree $T$ from an exponential distribution $p(.)$, the tree $T$ is then $\lambda$-random (spanning) for $\lambda_e := e^{\gamma_e} \ \ \forall e \in E$.


%Section 2.6 Sampling a Random Tree%
\subsection{$\lambda$-Random (Spanning) Trees}
To sample a $\lambda$-tree, we will utilize an iterative approach similiar to \cite{VGK90} so that our approach will have a polynomial running time for general $\lambda_e$'s. The basic idea is to order the edges $e_1,...,e_m$ of $G$ arbitrarily and then iteratively process the edges by deciding probabilistically whether or not to add the edge to the final tree or discard the edge. Essentially, when processing the i-th edge $e_i$, we decide whether or not to add it to the final spanning tree $T$ based on the probability $p_i$, which is based off the probability that $e_i$ is in a $\lambda$-random tree conditioned on the past decisions that were made for edges $e_1,...,e_{i-1}$ in past iterations. To compute these probabilities efficiently, first note that $p_1 = z_{e1}$. If we choose to include $e_1$ in the tree, then
\begin{equation*}
p_2 = Pr[e_2 \in T | e_1 \in T] = \frac{\sum\limits_{T' \ni e_1, e_2}\prod\limits_{e \in T'}\lambda_e}{\sum\limits_{T' \ni e_1}\prod\limits_{e \in T'}\lambda_e}
\end{equation*}
\begin{equation*}
= \frac{\sum\limits_{T' \ni e_1, e_2}\prod\limits_{e \in T'\\e_1}\lambda_e}{\sum\limits_{T' \ni e_1}\prod\limits_{e \in T'\\e_1}\lambda_e}.
\end{equation*}
This tells us that the probability of $e_2 \in T$ on the condition that $e_1 \in T$ is equal to the probability that $e_2$ is in a $\lambda$-random tree of a graph given by $G$ by contracting edge $e_1$. If we discard $e_1$, then the probability $p_2$ is equal to the probability that $e_2$ resides in a $\lambda$-random tree of a graph given by $G$ by removing $e_1$. Thus, for the general case, we can say that $p_i$ is equal to the probability that edge $e_i$ resides in a $\lambda$-random tree of a graph given by $G$ by contracting all edges that we decided to add to the tree and deleting all the edges that we decided to discard. Thus, to get each $p_i$, we compute the probability $p_{G'}[\lambda,f]$ that some edge $f$ is in a $\lambda$-random tree of a given multigraph $G^{'}$ and values of $\lambda_e$'s by noting that $p_{G'}[\lambda,f]$ is equal to $\lambda_f$ times the effective resistence of $f$ in $G'$ treated as an electrical circuit with conductances of edges given by $\lambda$ (\cite{LP14}, Ch. 4). Intuitively, $p_{G'}[\lambda,f]$ will be negatively correlated with resistance. The effective resistance is computed by the method given by \cite{GBS08}, Section 2.4, where, in simplistic terms, involves inverting some matrix that is derived from the Laplacian of $G'$. The Laplacian L is where
\begin{equation*}
L_{i,j} = 
  \begin{cases}
    -\lambda_e \indent \indent \indent \ \ e = (i,j) \in E \\
    \sum\limits_{e \in \delta(\{i\}}\lambda_e \indent \indent i = j \\
    0 \indent \indent \indent \indent \ otherwise.
  \end{cases}
\end{equation*}
To put it in words, the Laplacian entry $L_{i,j}$ is equal to the negative $\lambda$ weight of edge $e$ assuming there exists an edge $e = (i, j) \in E$, or equal to the sum of all the $\lambda$ weights of all incoming and outgoing edges of the vertice $i \in V$, or it is 0 otherwise. \newline
\indent A concentration bound is key to establishing the thinness of a sampled tree. Thus, we derive the concentration bounds with the following theorem:
\newline
\newline
\textbf{Theorem 2.6 (see \cite{AGM10}, section 5):} For each edge $e$, let $X_e$ be an indicator random variable associated with the event $[e \in T]$, where $T$ is a sampled $\lambda$-random tree. Also, for any subset $C$ of the edges of $G$, we define $X(C) = \sum\limits_{e \in C}X_e$. Then we have
\begin{equation*}
Pr[X(C) \geq (1 + \delta)E[X(C)]] \leq \bigg(\frac{e^{\delta}}{(1 + \delta)^{1 + \delta}}\bigg)^{E[X(C)]}.
\end{equation*}
Usually, in order to obtain concentration bounds, we would prove that the variables $\{X_e\}_E$ are independent and use Chernoff bounds. But, our variables $\{X_e\}_E$ are not independent. Rather, they are negatively correlated since our probability distribution is in product form (\cite{LP14}, Ch. 4). Thus, we get the following lemma:
\newline
\newline
\textbf{Lemma 2.7 (\cite{AGM10}, Section 5):} The indicator random variables $\{X_e\}_E$ are negatively correlated.
\newline
\newline
Now, since we have determined that the random variables are negatively correlated, we can use the fact that the upper tail part of the Chernoff bound only requires negative correlation \cite{PS97}.

%Section 2.7 O(log n /log log n) Thinness of the Sampled Tree%
\subsection{O(log n / log log n) Thinness of the Sampled Tree}
We first define $\tilde{p}(.)$ as the exponential distribution that we get from applying Theorem 2.5 to $\textbf{z}^*$. We prove that a sampled tree from $\tilde{p}(.)$ is almost guaranteed to be "thin". But first, we prove that if we look at a particular cut, the corresponding sampled tree will have the $\alpha$-thinness property with high probability where $\alpha = O(\frac{logn}{loglogn})$.
\newline
\newline
\textbf{Lemma 2.6 (\cite{AGM10}, Section 6):} If $T$ is a spanning tree sample from distribution $\tilde{p}(.)$ for $\epsilon = 0.2$ in a graph $G$ with $n \geq 5$ vertices then, for any subset $U \subset V$,
\begin{equation*}
Pr[|T \cap \delta(U)| > \beta z^*(\delta(U))] \leq n^{-2.5z^*(\delta(U))},
\end{equation*}
where $\beta = 4 logn / loglogn$.
\newline
\newline
\textbf{Proof (\cite{AGM10}, Section 6):} $\forall e \in E, \tilde{z}_e \leq (1 + \epsilon)z_{e}^{*}$, where $\epsilon = 0.2$ is our desired accuracy of approximation of $\textbf{z}^*$ by $\tilde{z}$ as given in section 2.5, Theorem 2.5. Thus
\begin{equation*}
E[|T \cap \delta(U)|] = \tilde{z}(\delta(U)) \leq (1 + \epsilon)z^*(\delta(U)).
\end{equation*}
Applying Theorem 2.6 with
\begin{equation*}
1 + \delta = \beta \frac{\textbf{z}^*(\delta(U))}{\tilde{z}(\delta(U))} \geq \frac{\beta}{1 + \epsilon}
\end{equation*}
we get $Pr[|T \cap \delta(U)| > \beta \textbf{z}^*(\delta(U))]$ which can be bounded from above by
\begin{equation*}
Pr\Big[|T \cap \delta(U)| > (1 + \delta)E[|T \cap \delta(U)|\Big] \leq \bigg(\frac{e^{\delta}}{(1 + \delta)^{1 + \delta}}\bigg)^{\tilde{z}(\delta(U))} \leq \bigg(\frac{e}{1 + \delta}\bigg)^{(1 + \delta)\tilde{z}(\delta(U))}
\end{equation*}
\begin{equation*}
= \bigg(\frac{e}{1 + \delta}\bigg)^{\beta z^*(\delta(U))} \leq \Bigg[\bigg(\frac{e(1 + \epsilon)}{\beta}\bigg)^\beta\Bigg]^{z^*(\delta(U))} \leq n^{-2.5z^*(\delta(U))}
\end{equation*}
Where in the last inequality, we did
\begin{equation*}
log\Bigg[\bigg(\frac{e(1 + \epsilon)}{\beta}\bigg)^\beta\Bigg] = 4\frac{logn}{loglogn}[1 + log(1 + \epsilon) - log(4) - log log n + logloglogn]
\end{equation*}
\begin{equation*}
\leq -4logn\bigg(1-\frac{logloglogn}{loglogn}\bigg) \leq -4\bigg(1 - \frac{1}{e}\bigg)logn \leq -2.5logn,
\end{equation*}
\newline
because $e(1 + \epsilon) < 4$ and $\frac{logloglogn}{loglogn} \leq \frac{1}{e}$ for all $n \geq 5$. \hfill\qed
\newline
Now we can combine the concentration results along with the union-bounding technique of Karger \cite{Kar93} to get the desired thinness of a sampled tree.
\newline
\newline
\textbf{Theorem 2.7 (\cite{AGM10}, Section 6):} Let $n \geq 5$ and $\epsilon = 0.2$. Let $T_1,...,.T_{\lceil 2logn \rceil}$ be $\lceil 2logn \rceil$ independent samples from a distribution $\tilde{p}(.)$ as given in Theorem 2.5. Let $T^*$ be the tree among these samples that minimizes the cost $c(T_j)$. Then, with high probability, $T*$ is $(4log n / log log n, 2)$-thin with respect to $\textbf{z}^*$.
\newline
We say high probability is a probability that is at at least $1 - \frac{1}{n - 1}$.
\newline
\newline
\textbf{Proof (\cite{AGM10}, Section 6):} First, by Lemma 2.6, the probability that some cut $\delta(U)$ violates the $\beta$-thinness of $T_j$ is at most $n^-2.5z^*(\delta(U))$. This shows that for any tree $T_j$ where $1 \leq j \leq \lceil 2logn \rceil$, $T_j$ is $\beta$-thin with high probability for $\beta = 4logn / loglogn$. \newline
\indent Now we show that there are at most $n^{2l}$ cuts of size at most l times the minimum cost value for any half-integer $l \geq 1$ \cite{Kar93}. By the Held-Karp relaxation and $\textbf{z}^*$, $\textbf{z}^*(\delta(U)) \geq 2(1 - 1/n)$, which shows there is at most $n^l$ cuts $\delta(U)$ with $\textbf{z}^*(\delta(U)) \leq l(1 - 1/n)$ for any integer $l \geq 2$. Thus, applying union bound \cite{Kar93} along with $n \geq 5$, we find that the probability that there exists a cut $\delta(U)$ with $|T_j \cap \delta(U)| > \beta \textbf{z}^*(\delta(U))$ is at most
\begin{equation*}
\sum\limits_{i = 3}^{\inf}n^in^{-2.5(i-1)(1-1/n)} \leq \sum\limits_{i = 3}^{\inf}n^{-i + 2} = \frac{1}{n - 1}
\end{equation*}
showing that there exists a cut where the $\beta$-thinness property is violated by $T^*$ is at most $1 - \frac{1}{n - 1}$. The excepted cost of $T_j$ is
\begin{equation*}
E[c(T_j)] \leq \sum\limits_{e \in E}\tilde{z}_e \leq (1 + \epsilon)\frac{n - 1}{n}\sum\limits_{a \in A}x_{a}^{*} \leq (1 + \epsilon)OPT_{HK}
\end{equation*}
From here, we can use Markov's inequality $(Pr[X \geq a] \leq E(X)/a)$ to say that for any $T_j$, the probability that $c(T_j) > 2OPT_{HK}$ is at most $(1 + \epsilon)/2$, which shows that with a probability at most $(\frac{1+\epsilon}{2})^{2logn} < 1/n$ for $\epsilon = 0.2$, we have $c(T^*) > 2OPT_{HK}$, thus proving the theorem. \hfill\qed
\newline
\newline
Now we can prove that there is $O(logn / loglogn)$-approximation factor solution to ATSP.
\newline
\newline
\textbf{Theorem 2.8 (\cite{AGM10}, Section 6):} Algorithm 1 finds a $(2 + 8logn / loglogn)$-approximate solution to the Asymmetric Traveling Salesman Problem with high probability and in time that is polynomial in the size of the input.
\newline
\newline
\textbf{Proof:} We start with step 1 of the algorithm to find an optimal extreme-point solution $\textbf{x}^*$ to the Held-Karp LP relaxation of ATSP. In section 2.2, we detailed that the value of $\textbf{x}^* = OPT_{HK}$. We then define $\textbf{z}^*$ by (5) in section 2.3. \newline
\indent Next, in step 2 of the algorithm, we use Theorem 2.5 on $\textbf{z}^*$, along with $\epsilon = 0.2$, to get the weights $\{\tilde{\gamma}\}_{e \in E}$. This runs in polynomial time because since $\textbf{x}^*$ was an extreme point, then $z_{min}^{*} \geq e^{-O(nlogn)}$. \newline
\indent For step 3 of the algorithm, we use the sampling method detailed in section 2.6 to sample $2\lceil logn \rceil$ spanning trees in polynomial time. And from Theorem 2.7, we know that $T^*$ is $(4logn / loglogn, 2)$-thin with high probability. \newline
\indent Lastly, in step 4 of the algorithm, we use Theorem 2.3 to get a $((8logn / loglogn) + 2)$-approximation of the ATSP instance. This concludes the proof of this theorem. \hfill \qed

%%%%%%%%%%%%%%%%%%%%Section 3 An Inapproximability Bounds of ATSP%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Inapproximability Bounds of ATSP}
In this section, we will detail the tools and methods that Karpinski et al. used to derive the 75/74 inapproximability bound of ATSP. We will not be diving into proofs of any theorems; however, we will use the theorems provided in the paper to explain the process of deriving the 75/74 inapproximability bound.

\subsection{High Level Overview of Algorithm}
The problem starts from the MAX-E3-LIN2 problem where, given a system of linear equations mod 2 with exactly 3 variables in each equation, we want to find an assignment of the variables that maximizes the number of satisfied equations. A reduction from MAX-E3-LIN2 to a special case of MAX-E3-LIN2 is done where each variable appears exactly 3 times and the linear equations have a particular structure. This reduction is done using a tool called a ``bi-wheel amplifier''. The special case of MAX-E3-LIN2 then gets reduced to ATSP by constructing and utilizing ``gadgets'' in a careful manner.

\subsection{Bi-wheel Amplifiers}
Amplifier graphs are useful in proving inapproximability for CSPs where every variable appears a bounded number of times. The bi-wheel amplifiers are a variation of the wheel amplifier graphs \cite{BK01}, which generalizes the concept of expander graphs and formalizes the construction based from Papadimitriou and Yannakakis \cite{PY91}. A wheel amplifier is essentially a graph with edge expansion 1 for a subset of its vertices. Edge expansion, denoted h(G), is calculated by $h(G) = \min_{0 < |S| \leq n/2}\frac{|\delta S|}{|S|}$ where $S$ is a subset of vertices of $V$ of the graph $G$ and $\delta S$ are the outgoing edges of $S$. \newline
\indent 3-regular wheel amplifiers are used within this algorithm. To construct a 3-regular wheel amplifier, you first start with a cycle on $7n$ vertices ($n$ is the number of nodes in the set). Every 7th vertex is considered a contact vertex. All other vertices are considered checkers. You then select a random perfect matching of checkers. The key property of this 3-regular wheel amplifier is that, with high probability, any partition of vertices cuts more edges than the number of contact vertices in the smaller set. The bi-wheel amplifier consists of two 3-regular wheel amplifiers, connected in a way that encodes inequality constraints in an efficient manner.

\subsection{Hybrid Problem}
The Hybrid Problem is essentially the special space of the MAX-E3-LIN2 problem where each variable appears exactly 3 times, but it is not necessarily that each equation has exactly 3 variables. The ideal Hybrid Problem to be used for reduction to ATSP is given by the Theorem 3 of the paper:
\newline
\newline
\textbf{Theorem 3 \cite{KLS15}:} For every constant $\epsilon > 0$ and $b \in {0,1}$, there exist instances of the Hybrid problem with 31m equations such that: \newline
(i) Each variable occurs exactly three times. \newline
(ii) 21m equations are of the form $x \oplus y = 0$, $9m$ equations are of the form $x \oplus y = 1$ and m equations are of the form $x \oplus y \oplus z = b$. \newline
(iii) It is NP-hard to decide whether there is an assignment to the varables that leaves at most $\epsilon \cdot m$ equations unsatisfied, or every assignment to the variables leaves at least $(0.5 - \epsilon)m$ equations unsatisfied (this is based on Theorem 2 of the paper, which originated from H$\mathring{a}$stad \cite{Has01}).

\subsection{Reduction to ATSP}
The reduction from the Hybrid problem to ATSP is through the use of ``gadgets''. Gadgets are essentially a cluster of weighted edges where the vertices between the edges that correlate with the variables of the CSPs. ``Careful'' tinkering of edge weights will determine the path that the truth assignment of a variable will take. One such tinkering technique is the notion of ``forced edges'' whereby adding intermediate vertices within an edge to distribute the weight of the edge, it ``forces'' an edge to be taken; however, a variable should only take exactly one forced edge in order to have an ``honest traversal''. Having a ``dishonest traversal'' correlates with paying all unsatisfied equations so we want to maximize honest traversals. After getting the optimal reduction, the proof of Theorem 5 of the paper states that ``$\forall \epsilon > 0$, it is NP-hard to tell whether there is a tour with cost at most $37m' + 5v + 2m(v + \lambda) + \epsilon \cdot m' \leq 37 \cdot m' + \epsilon'm'$ or that all tours have cost at least $37m' + (0.5 - \epsilon)m' \geq 37.5 \cdot m' - \epsilon' \cdot'$, for some $\epsilon'$ depending only on $\epsilon, \delta, \lambda$ \cite{KLS15}.'' Thus, we can see that the ratio between the two cases gets arbitrarily close to 75/74 for appropriate choices of $\epsilon, \delta, \lambda$, which gives us the inapproximability bounds.

%%%%%%%%%%%%%%%%%%%%Section 4 Conclusion%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Concluding Remarks}
We briefly describe some of the recent work for special cases of ATSP.  Gharan and Saberi showed that there is a constant factor approximation on ATSP of planar graphs and graphs with bounded genus using a tool called threads, along with thin-trees \cite{GS11}. Anari and Gharan used a tool called spectrally-thin trees (a variant of thin-trees) to show that the integrality gap of ATSP is $O$(poly log log $n$) \cite{AG15}, but there is no current method known to construct a solution of $O$(poly log log $n$)-approximation. The recent work of Svensson et al. showcased a new heuristic called ``Local-Connectivity'' for tackling approximating ATSP \cite{Sve15}, with a constant factor approximation on a special case of ATSP that contains only two different edge weights \cite{STV16}. \newline
\indent In my opinion, seeing constant factor approximations for any variant of ATSP shows that there must be some algorithm or heuristic waiting to be discovered to get the constant factor approximation for the general case. I am especially keen on the ``Local-Connectivity'' heuristic that Svennson et al. \cite{Sve15} since it deals with a partial relaxation of the connectivity constraint given by the Held-Karp LP relaxation to create clusters of the vertices of G. It reminds me of 

\nocite{*}
\bibliographystyle{acm}
\bibliography{references}
\end{document}